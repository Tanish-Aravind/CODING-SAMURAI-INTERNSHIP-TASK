  # CODING-SAMURAI-INTERNSHIP-TASK
Completed machine learning internship tasks (Batch A-45) from Coding Samurai – includes regression, classification, NLP, and deep learning projects.

Task 1 : House Price Prediction – Summary
In this project, I built a Linear Regression model to predict house prices using a dataset containing various property features like area, number of bedrooms, availability of a guest room, air conditioning, and more. I began with essential data preprocessing, where I handled binary categorical features (yes/no) using mapping, and applied OneHotEncoding to multi-class categorical variables. After cleaning the data, I used a ColumnTransformer to ensure smooth preprocessing of categorical and numerical data.
To enhance model performance and stabilize variance, I applied a log transformation on the target variable (price) before training the model. Then, I trained a LinearRegression model using train-test split, predicted log-scaled prices, and inverse transformed them back using np.expm1() for meaningful interpretation. The model’s performance was evaluated using Mean Squared Error (MSE) and R² Score, achieving an R² of ~0.66, which indicates a fairly strong linear relationship between features and house price. The results show the model's potential in real-world price prediction scenarios.

Task 2 : Iris Flower Classification – Summary
In this task, I developed a classification model to predict the species of an Iris flower based on features like petal length, petal width, sepal length, and sepal width. I began by exploring the Iris dataset, performing basic data visualization using plots like scatter plots and pair plots to understand the relationship between features and classes. Then, I split the dataset into training and testing sets and built a supervised machine learning model, typically using algorithms like Logistic Regression, K-Nearest Neighbors (KNN), or Decision Tree for multi-class classification.
After training, I evaluated the model's performance using metrics like accuracy, confusion matrix, and classification report to assess how well it distinguishes between the three flower species – Setosa, Versicolor, and Virginica. The model achieved high accuracy due to the dataset's simplicity and separability, making it an ideal starter project to grasp the core concepts of classification and model evaluation.

Task 3 : Sentiment Analysis on Tweets – Summary
In this project, I worked on classifying tweets based on their sentiment — Positive, Negative, or Neutral — using Natural Language Processing (NLP) techniques. I began by cleaning the raw text data, which involved removing punctuation, converting to lowercase, eliminating stopwords, and applying techniques like tokenization and stemming/lemmatization to normalize the text. This step ensured that the model could focus on the actual meaning of the text rather than noise.
After preprocessing, I used text vectorization techniques like TF-IDF or CountVectorizer to convert text into numerical form. Then I trained a classification model — typically Logistic Regression, Naive Bayes, or a similar algorithm — to learn patterns in the sentiment labels. The model was evaluated using metrics like accuracy, precision, recall, and confusion matrix, giving insights into its ability to differentiate between tweet sentiments. This project gave me hands-on experience in real-world text classification problems and a solid grip on the NLP workflow.

Task 4 : Handwritten Digit Recognition – Summary
In this project, I built a machine learning model to recognize handwritten digits (0–9) using the MNIST dataset, which contains thousands of grayscale images of digits. I started by loading and visualizing the data to understand its structure — 28x28 pixel images flattened into feature vectors. Then, I normalized the pixel values to improve training efficiency and prevent the model from getting biased due to varying scales.
I used a neural network model, most likely with frameworks like Keras or TensorFlow, comprising layers like Dense, ReLU activations, and Softmax output for multi-class classification. After training the model using a portion of the dataset, I evaluated its performance on the test set using accuracy score and confusion matrix, which showed strong classification performance. This task was a great intro to deep learning concepts like feedforward networks, backpropagation, and model evaluation using image data.

Task 5 : The task in this project is to build a machine learning model that can predict customer churn — whether a customer is likely to leave a service. Since a real-world dataset isn't used, synthetic data resembling telecom customer records is generated, including features like tenure, monthly charges, internet service type, and support availability. The churn label (target variable) is created using a logistic formula based on some of these features to simulate realistic customer behavior patterns.
Once the dataset is prepared, the workflow includes preprocessing (like feature engineering and encoding categorical variables), followed by model training using a Random Forest Classifier. The model undergoes hyperparameter tuning through cross-validation with GridSearchCV to improve performance. After training, the model is evaluated using classification metrics (accuracy, F1-score, ROC AUC), and its predictions are explained using feature importance plots and SHAP values. The final model is saved for future use, completing a full end-to-end classification pipeline.
